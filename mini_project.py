# -*- coding: utf-8 -*-
"""mini project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15SlvXi8BtPldH5VN7mfup22rSznVIEAa
"""

#mount drive
from google.colab import drive
drive.mount('/content/drive')

# !wget https://drive.google.com/file/d/11w9g1io_Prtg-zQItbsTPuKp5-6MehrV/view?usp=share_link
# !unzip /content/view?usp=share_link

#https://drive.google.com/file/d/1Q72GnHLXPti7WmstHPENHKbjl9HsC-_C/view?usp=share_link
!unzip "/content/drive/MyDrive/archive (1).zip" -d "/content/drive/MyDrive/"

#pandas and numpy are used for data manipulation and numerical computations, respectively.

import pandas as pd  #pandas is used to load and read the data in the form of images stored in a directory.
import numpy as np   # used for image preprocessing operations e.g. to convert the loaded images into arrays
import matplotlib.pyplot as plt
# used to define and train the deep learning model for image classification
import tensorflow as tf 

import os
#Keras is a high-level neural networks API, written in Python and capable of running on top of several lower-level deep learning frameworks such as TensorFlow
import keras

#generate batches of image data with real-time data augmentation during the training of a deep learning model
#takes the input of the directory containing the images, applies some predefined transformations, and returns batches of images in a format that can be used to train a deep learning model.
from keras.preprocessing.image import ImageDataGenerator

#loads an image from a file and returns it as a PIL (Python Imaging Library) image object
from keras.utils.image_utils import load_img
#takes the PIL image object as input and returns a Numpy array representation of the image used for input for training and prediction.
from keras.utils.image_utils import img_to_array

#VGG19 is a convolutional neural network architecture
#VGG19 model is loaded using the Keras VGG19 function, pre-trained model used as a feature extractor to obtain features from the input images
#preprocess_input: performs preprocessing steps on the input image to ensure that it is in the proper format for the VGG19 model to process.
from keras.applications.vgg19 import VGG19, preprocess_input, decode_predictions

#zoom=0.5: images can be scaled down to half of their original size or scaled up to double their original size
#shear=0.3: images can be sheared by up to 30% of their width or height so it helps distorted images to be recognised
#horizontal_flip=true:  involves flipping the image horizontally, which results in a mirror image of the original
#preprocessing_function=preprocess_input: 1. Rescales the pixel values of the image to be in the range of -1 to +1, 2. Applies mean subtraction to the image by subtracting the mean pixel value of the ImageNet dataset from each pixel.
#3. Divides each pixel by the standard deviation of the ImageNet dataset.
train_datagen= ImageDataGenerator(zoom_range=0.5 , shear_range=0.3  , horizontal_flip = True,preprocessing_function=preprocess_input)
val_datagen=ImageDataGenerator(preprocessing_function= preprocess_input)

#target_size=(256,256) , batch_size=32: helps to save memory and allows for efficient loading of the large image dataset

train=train_datagen.flow_from_directory(directory= '/content/drive/MyDrive/archive (1)/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/train' , target_size=(256,256) , batch_size=32)

val=val_datagen.flow_from_directory(directory= '/content/drive/MyDrive/archive (1)/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/valid' , target_size=(256,256) , batch_size=32)

t_img, label=train.next()

#plotImage() takes in an array of images and their corresponding labels and plots the images and their labels using matplotlib
def plotImage(img_arr, label):
  for im, l in zip(img_arr, label):
    plt.figure(figsize=(5,5))
    plt.imshow(im)
    plt.show()

#plotting first 3 images
plotImage(t_img[:3], label[:3])

"""#BUILDING OUR MODEL"""

#Dense: a layer type that specifies a fully connected layer in a neural network
#Flatten: a layer type that flattens the output of the preceding layer into a 1D array, which can then be fed into a dense layer
from keras.layers import Dense, Flatten
#Model: a class that provides a flexible way to define a neural network model by specifying its inputs and outputs, and the layers in between
from keras.models import Model
#VGG19: a pre-trained CNN model that has been trained on a large image dataset, and can be used for transfer learning in other image classification tasks
from keras.applications.vgg19 import VGG19
import keras

#Create an instance of the VGG19 model with an input shape of (256,256,3) and exclude the top (fully connected) layers of the model.
#top layer excluded because we want to add our own fully connected layers to perform classification on our own dataset
# VGG19 model expects to receive input images with a resolution of 256x256 pixels and 3 color channels
base_model=VGG19(input_shape=(256,256,3), include_top=False)

#freeze the weights of all the layers in the VGG19 architecture i.e. these layers will not be updated during the training process
for layer in base_model.layers:
  layer.trainable=False

# convert the output feature maps of the convolutional layers into a 1D array, which can be fed into the fully connected layers
X=Flatten()(base_model.output)

#layer will predict the probability distribution of the image belonging to each of the 38 classes in the dataset
#softmax activation function: for multiclass classification problems
#it takes a set of arbitrary inputs and transforms them into a set of outputs that represent the probability of each input(between 0 and 1) belonging to each class
X=Dense(units=38, activation='softmax')(X)


#Creating our model
#Keras Model instance with the VGG19 model's input and the output from the dense layer as the output
model=Model(base_model.input,X)

#Print the summary of the model, which includes the layer names, output shapes, and number of parameters
model.summary()


#convolutional layers: perform convolutional operations on the input images to extract relevant features
#MaxPooling2D layers: reduce the size of the output from the convolutional layers, help to prevent overfitting by reducing the number of parameters in the model, dimensionality reduction

#optimizer: to minimize the error between predicted output and actual output
# optimizer will adjust the weights and biases of the VGG19 model during training to minimize the categorical cross-entropy loss function
#loss function: to minimize the difference between the predicted probability distribution and the true probability distribution
#categorical_crossentropy: for multiclass classification
#accuracy; tells us how well the model is performing in terms of correctly predicting the class labels of the input images.
model.compile(optimizer = 'adam' , loss=keras.losses.categorical_crossentropy , metrics= ['accuracy'])

"""#Early Stopping and Model Checkpoint

"""

from keras.callbacks import ModelCheckpoint, EarlyStopping

#early stopping
# used to prevent overfitting and improve the performance of the model

#val_accuracy will be monitored to check if the validation accuracy is improving
#If it doesn't improve by at least min_delta for patience number of epochs, the training process will stop
#verbose: Controls the verbosity of the output
es=EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=3, verbose=1)

#model checkpoint
#ModelCheckpoint callback is used to save the best model based on the validation accuracy metric

#filepath: specifies the path where the model checkpoint files will be saved
#save_best_only: boolean value that indicates if the model should only be saved when the metric specified in the monitor argument improves
#.h5: file format commonly used in ML applications to store and load models' weights
mc=ModelCheckpoint(filepath="best_model.h5", monitor='val_accuracy', min_delta=0.01, patience=3, verbose=1, save_best_only=True )

cb=[es, mc]

#train a Keras model using a generator to supply the training and validation data, applies callbacks during training to monitor the model's progress, save the best weights to a file
#steps_per_epoch: the number of batches of samples to use in each epoch( In this case, there are 16 batches of samples per epoch)
#epochs: iterations
his=model.fit_generator(train, steps_per_epoch=16, epochs=50, verbose=1, callbacks= cb , validation_data=val , validation_steps = 16)
#if does not work then change the rntime settings to GPU

h=his.history
h.keys()
#h.keys() is used to print the keys of the dictionary which includes:

# 'loss': the training loss for each epoch
# 'accuracy': the training accuracy for each epoch
# 'val_loss': the validation loss for each epoch
# 'val_accuracy': the validation accuracy for each epoch
dict_keys=(['loss' , 'accuracy' , 'val_loss' , 'val_accuracy'])

plt.plot(h['accuracy'])
plt.plot(h['val_accuracy'] , c="red")
plt.title("acc vs v-acc")
plt.show()
#If both curves (training and validation accuracy) are close and increasing with epochs, it implies that the model is performing well

plt.plot(h['loss'])
plt.plot(h['val_loss'] , c="red")
plt.title("loss vs v-loss")
plt.show()
#If both curves (training and validation loss) are close and dereasing with epochs, it implies that the model is performing well

#load best model

from keras.models import load_model

model = load_model("/content/best_model.h5")#dataset best_model.h5

#evaluate_generator method is used to evaluate the model's performance on the validation dataset
#[0]: loss ; [1]: accuracy
acc =model.evaluate_generator(val)[1]
print(f"the accuracy of your model is = {acc} %")

#list(train.class_indices.values()) returns a list of the numeric labels
#list(train.class_indices.keys()) returns a list of the corresponding class labels
#zip(): pairs the numeric labels with their corresponding class labels. 
ref=dict(zip(list(train.class_indices.values()), list(train.class_indices.keys())))
#necessary because the model.predict() method returns predicted class indices, and we want to convert them back to class names for interpretation

def prediction(path):
  #takes as input the file path of the image and the target size that we want to resize the image to.

  #load_img(): used to load the image from the file path
  #np.expand_dims(): used to add an extra dimension to the img numpy array to create a batch of 1 img, which can be inputted into the model for prediction
  img=load_img(path,target_size=(256,256))
  #img_to_array(): used to convert the image to a numpy array
  i=img_to_array(img)
  #preprocess_input(): used to preprocess the image based on the pre-trained model requirements
  im=preprocess_input(i)
  #np.expand_dims(): used to add an extra dimension to the img numpy array to create a batch of 1 img, which can be inputted into the model for prediction
  img=np.expand_dims(im,axis=0)
  #model.predict(): called on the image array to get the predicted class probabilities for the image
  #np.argmax(): used to get the index of the highest probability class, which corresponds to the predicted class for the image
  pred = np.argmax(model.predict(img))
  
  #predicted class is obtained from the ref dictionary
  print(f" the image belongs to {ref[pred]} ")

#prediction on test data
path="/content/drive/MyDrive/archive (1)/test/test/AppleCedarRust4.JPG"
prediction(path)

